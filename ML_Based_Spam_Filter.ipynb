{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_Based_Spam_Filter.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMelcv9D6oJKurw1WedeNv1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Karishma-Kuria/CMPE-256-Adv-DataMining/blob/main/ML_Based_Spam_Filter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNsMR0KlVy0x"
      },
      "source": [
        "# **ML Based Spam Filter**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JfTfVd6NlZz"
      },
      "source": [
        "# Importing relevant Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import string"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "id": "LPAlPsqPOTMp",
        "outputId": "60934e10-36aa-4f9f-ff14-02be01b0edb4"
      },
      "source": [
        "# Reading the csv file containg the spam documents\n",
        "dataset_path = 'https://github.com/Karishma-Kuria/CMPE-256-Adv-DataMining/blob/main/SpamDoc.csv?raw=true'\n",
        "ds = pd.read_csv(dataset_path)\n",
        "ds.head()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>spam</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Free -Coupons for next movie. The above links ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Free -Coupons for next movie. The above links ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Our records indicate your Pension is under per...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I know that's an incredible statement, but bea...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Dear recipient, Avangar Technologies announces...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  spam\n",
              "0  Free -Coupons for next movie. The above links ...     1\n",
              "1  Free -Coupons for next movie. The above links ...     1\n",
              "2  Our records indicate your Pension is under per...     1\n",
              "3  I know that's an incredible statement, but bea...     1\n",
              "4  Dear recipient, Avangar Technologies announces...     1"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqdhTqisQy4Z"
      },
      "source": [
        "Since all thes documents are spams so I have assigned 1 in the spam column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HinmG8HxQlz2",
        "outputId": "000f06df-e73b-4247-f1f1-15754406b305"
      },
      "source": [
        "ds.shape"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKfw5-1WRBzL",
        "outputId": "ce7707ec-f9da-47e1-b465-0c694613cb71"
      },
      "source": [
        "# Removing Duplicates\n",
        "ds.drop_duplicates(inplace=True)\n",
        "print(ds.shape)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNej0defRKmc",
        "outputId": "3cad1f98-009a-46db-f3ee-54c9c88db217"
      },
      "source": [
        "# Downloading the NLTK stopwords package\n",
        "nltk.download(\"stopwords\")"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C96LdRjkRSBR"
      },
      "source": [
        "Here I have used functions to do cleaning of the data. It will clean the text in dataset and return the tokens. In cleaning it will first remove punctuations and then remove the stop words ex: is, the etc\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zc125ioyRrgT",
        "outputId": "b6fe51e2-58a0-4670-b8dd-643dd80fd917"
      },
      "source": [
        "# For Tokenization (a list of tokens), will be used as the analyzer\n",
        "\n",
        "def cleaning_process(text):\n",
        "\n",
        "#1 Remove Punctuationa\n",
        "    no_punctuation = [char for char in text if char not in string.punctuation]\n",
        "    no_punctuation = ''.join(no_punctuation)\n",
        "\n",
        "#2 Remove Stop Words\n",
        "    clean = [word for word in no_punctuation.split() if word.lower() not in stopwords.words('english')]\n",
        "    return clean\n",
        "\n",
        "\n",
        "# to show the tokenization\n",
        "ds['text'].head().apply(cleaning_process)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [Free, Coupons, next, movie, links, take, stra...\n",
              "2    [records, indicate, Pension, performing, see, ...\n",
              "3    [know, thats, incredible, statement, bear, exp...\n",
              "4    [Dear, recipient, Avangar, Technologies, annou...\n",
              "5    [Enter, win, 25000, get, Free, Hotel, Night, c...\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7Ixg3DdSHzj"
      },
      "source": [
        "Feature Engineering : Feature Extraction: removing non relevant features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoHN73n8SPEk"
      },
      "source": [
        "# Here I am converting the text into a matrix containing token counts :\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cleaning_message = CountVectorizer(analyzer=cleaning_process).fit_transform(ds['text'])"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtHXAp2HSeJ8",
        "outputId": "5941924a-80d5-4065-9683-f7b96f9b1e10"
      },
      "source": [
        "# Spliting the data into training and testing\n",
        "# 70%: Training and 30%: Testing\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(cleaning_message, ds['spam'], test_size=0.30, random_state=0)\n",
        "# Checking shape of the data\n",
        "print(cleaning_message.shape)\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5, 197)\n",
            "(3, 197)\n",
            "(2, 197)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eskkMQCXSv9D"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "tfidf_transformer = TfidfTransformer(smooth_idf=True,use_idf=True)\n",
        "\n",
        "x_train_transformed = tfidf_transformer.fit(xtrain)\n",
        "\n",
        "x_test_transformed = tfidf_transformer.fit(xtest)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwuxuusdTPBa",
        "outputId": "3b2ff39c-4b01-419c-9727-72634643f7e6"
      },
      "source": [
        "print(x_train_transformed)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRERj0N5TXdw"
      },
      "source": [
        "# Creating and training the Naive Bayes Classifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "classifier = MultinomialNB().fit(xtrain, ytrain)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bK4tnXJ0TnXj"
      },
      "source": [
        "Checking the Classifiers prediction and the actual value of Dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VpaMrtvT1f-",
        "outputId": "bf42b50c-17e2-43af-def5-0d55e002db03"
      },
      "source": [
        "print(classifier.predict(x_train))\n",
        "print(y_train.values)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 1 1]\n",
            "[1 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRMlu3aZUCfw"
      },
      "source": [
        "Now we'll check the performance of model by evaluating the Naive Bayes classifier and the report, confusion matrix & accuracy score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2eab-YuUNva",
        "outputId": "8225bcee-2454-42cb-9f25-15606b2e3cc7"
      },
      "source": [
        "# Evaluating model on the training data set\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "pred = classifier.predict(x_train)\n",
        "print(classification_report(y_train, pred))\n",
        "print()\n",
        "print(\"Confusion Matrix: \\n\", confusion_matrix(y_train, pred))\n",
        "print(\"Accuracy: \\n\", accuracy_score(y_train, pred))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       1.00      1.00      1.00         3\n",
            "\n",
            "    accuracy                           1.00         3\n",
            "   macro avg       1.00      1.00      1.00         3\n",
            "weighted avg       1.00      1.00      1.00         3\n",
            "\n",
            "\n",
            "Confusion Matrix: \n",
            " [[3]]\n",
            "Accuracy: \n",
            " 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctvxEsfXUhJx",
        "outputId": "0e5d05a6-228a-4095-a5c4-a468cd154c34"
      },
      "source": [
        "# Print the classifier Prediction\n",
        "print(classifier.predict(x_test))\n",
        "# Print the actual values\n",
        "print(y_test.values)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 1]\n",
            "[1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tm_64LSRUu7f"
      },
      "source": [
        "The above result shows that, it has predicted accurately on test data, spam =1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XD1DU3xmU42i",
        "outputId": "934d52a8-137e-4d53-8b0b-b7231d53849f"
      },
      "source": [
        "# Evaluating model on test data\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "predict = classifier.predict(x_test)\n",
        "print(classification_report(y_test, predict))\n",
        "print()\n",
        "print(\"Confusion Matrix: \\n\", confusion_matrix(y_test, predict))\n",
        "print(\"Accuracy: \\n\", accuracy_score(y_test, predict))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       1.00      1.00      1.00         2\n",
            "\n",
            "    accuracy                           1.00         2\n",
            "   macro avg       1.00      1.00      1.00         2\n",
            "weighted avg       1.00      1.00      1.00         2\n",
            "\n",
            "\n",
            "Confusion Matrix: \n",
            " [[2]]\n",
            "Accuracy: \n",
            " 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ax5ahAcvVN33"
      },
      "source": [
        "From the above result its clear that the classifier accurately identified the  messages as spam with 100% accuracy on the test data. Since we have only spam messages in Dataset"
      ]
    }
  ]
}